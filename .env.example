# Local LLM Configuration
# Copy this file to .env and customize as needed

# LLM Model Configuration
LLM_MODEL_NAME=mistral
LLM_BASE_URL=http://localhost:11434
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2048

# Agent Configuration
DEBUG=False

# MCP Configuration
ENABLE_MCP=True

# MCP Server Configurations
# Format: name1:command,name2:command
# Example: github:npx @modelcontextprotocol/server-github,filesystem:npx @modelcontextprotocol/server-filesystem
MCP_SERVERS=

# MCP-specific Environment Variables
GITHUB_TOKEN=
DATABASE_URL=

# Optional: For other LLM services
# LLM_BASE_URL=http://localhost:8000  # LM Studio
# LLM_BASE_URL=http://localhost:5000  # llama.cpp

